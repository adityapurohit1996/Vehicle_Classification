{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Run_Yolo_3D_BoundingBox_UMICH.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DNb4Q95V_689","colab_type":"code","outputId":"2b713cc3-ba44-4a16-f95e-9d7fe538987c","executionInfo":{"status":"ok","timestamp":1576264417060,"user_tz":300,"elapsed":1075,"user":{"displayName":"Nam Gyu Kil","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ5TS287X08iAcQu7bCatJvBOJr9-oiAfqj5Ox=s64","userId":"08278423732697661539"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"om75_dRnAIEB","colab_type":"code","outputId":"5b266b2a-3d2b-44f0-dab3-23e169a087db","executionInfo":{"status":"ok","timestamp":1576264482676,"user_tz":300,"elapsed":1210,"user":{"displayName":"Nam Gyu Kil","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ5TS287X08iAcQu7bCatJvBOJr9-oiAfqj5Ox=s64","userId":"08278423732697661539"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/drive/My Drive/Colab Notebooks/VehicleDetection\n","# %cd /content/drive/My Drive/Colab Notebooks/VehicleDetection/data-2019/test"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/VehicleDetection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QSLFOfVXA-2A","colab_type":"code","outputId":"81da9d02-4257-4294-b98e-47b04d7a964b","executionInfo":{"status":"ok","timestamp":1576264588629,"user_tz":300,"elapsed":28900,"user":{"displayName":"Nam Gyu Kil","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ5TS287X08iAcQu7bCatJvBOJr9-oiAfqj5Ox=s64","userId":"08278423732697661539"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import tensorflow as tf\n","from keras.models import load_model, Model\n","\n","yolov3 = load_model('keras-yolo3-master/yolo_ROB535.h5')\n","for layer in yolov3.layers:\n","    layer.trainable = False\n","\n","gmail_model = load_model('keras-yolo3-master/gmail_shorterlabels_10.h5')\n","for layer in gmail_model.layers:\n","    layer.trainable = False"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n","  warnings.warn('No training configuration found in save file: '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MzxOx9QUgYVT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":203},"outputId":"a3256963-4065-4b40-eede-2e0c647de4fe","executionInfo":{"status":"error","timestamp":1576264588630,"user_tz":300,"elapsed":26714,"user":{"displayName":"Nam Gyu Kil","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ5TS287X08iAcQu7bCatJvBOJr9-oiAfqj5Ox=s64","userId":"08278423732697661539"}}},"source":["from keras import backend as K\n","import cv2\n","import numpy as np\n","\n","net_h, net_w = 416, 416\n","\n","get_98th_layer_output = K.function([yolov3.layers[0].input],\n","                                    [yolov3.layers[98].output])\n","\n","layer_name = 'conv_105'\n","intermediate_layer_model = Model(inputs=gmail_model.input,\n","                                 outputs=gmail_model.get_layer(layer_name).output)\n","# intermediate_output = intermediate_layer_model.predict(data)"],"execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-e77718d78306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m intermediate_layer_model = Model(inputs=gmail_model.input,\n\u001b[1;32m     12\u001b[0m                                  outputs=gmail_model.get_layer(layer_name).output)\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"rxgAm4-wBLPd","colab_type":"code","colab":{}},"source":["# yolov3.trainable = False\n","# yolov3.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibheC1qBH5dR","colab_type":"code","colab":{}},"source":["def preprocess_input(image, net_h, net_w):\n","    new_h, new_w, _ = image.shape\n","\n","    # determine the new size of the image\n","    if (float(net_w)/new_w) < (float(net_h)/new_h):\n","        new_h = (new_h * net_w)//new_w\n","        new_w = net_w\n","    else:\n","        new_w = (new_w * net_h)//new_h\n","        new_h = net_h\n","\n","    # resize the image to the new size\n","    resized = cv2.resize(image[:,:,::-1]/255., (new_w, new_h))\n","\n","    # embed the image into the standard letter box\n","    new_image = np.ones((net_h, net_w, 3)) * 0.5\n","    new_image[(net_h-new_h)//2:(net_h+new_h)//2, (net_w-new_w)//2:(net_w+new_w)//2, :] = resized\n","    new_image = np.expand_dims(new_image, 0)\n","\n","    return new_image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nj93IfMHdhs","colab_type":"code","colab":{}},"source":["photo_filename = 'data4YOLO/image_folder/00107_image.jpg'\n","# photo_filename = 'data4YOLO/image_folder/00020_image.jpg'\n","# photo_filename = 'data4YOLO/image_folder/00058_image.jpg'\n","# load and prepare image\n","image = cv2.imread(photo_filename)\n","image_h, image_w, _ = image.shape\n","process_image = preprocess_input(image, net_h, net_w)\n","layer_output = get_98th_layer_output([process_image])[0]\n","layer_output = intermediate_layer_model.predict([process_image])[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRmuPqKWMZ0u","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models\n","from keras.models import Sequential\n","from tensorflow.keras import applications\n","from keras.layers import Dense, Dropout, Flatten\n","\n","# Use this with pretrained Model\n","def feedforward_network():\n","    model = models.Sequential()\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(2024, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(1024, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(512, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(256, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(64, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(32, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(3))\n","    model.compile(optimizer='adam',\n","              loss='mean_squared_error',\n","              metrics=['mean_squared_error'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E0ARkomPlnWh","colab_type":"text"},"source":["# Another attempt at 3D Model"]},{"cell_type":"code","metadata":{"id":"KWxLZ0eBlne6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"543dee11-9bc1-438e-e43f-451f795a0aa1"},"source":["import numpy as np\n","import csv\n","import cv2\n","import sklearn\n","import matplotlib.pyplot as plt\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras_preprocessing.image import ImageDataGenerator\n","from keras.preprocessing.image import img_to_array\n","from sklearn.model_selection import train_test_split\n","from random import shuffle\n","from math import ceil\n","from IPython.core.debugger import set_trace\n","from sklearn import preprocessing\n","from numpy import expand_dims\n","import pdb\n","\n","class Pipeline:\n","    def __init__(self, model=None, base_path=''):\n","        self.data = []\n","        self.model = model\n","        self.base_path = base_path\n","        self.epochs = 40\n","        self.batch_size = 128\n","        self.training_samples = []\n","        self.validation_samples = []\n","        self.csv_path = self.base_path + '/centroids.csv'\n","        self.model_path = 'models/'\n","\n","    def getSamples(self):\n","        print('Running getSamples functions')\n","        with open(self.csv_path) as csvfile:\n","            reader = csv.reader(csvfile)\n","            next(reader)\n","            count = 0\n","            lines = []\n","            for line in reader:\n","                lines.append(line)\n","                count += 1\n","                if (count == 3):\n","                    count = 0\n","                    self.data.append(lines)\n","                    lines = []\n","        return None\n","    \n","    def split_data(self):\n","        train_samples, validation_samples = train_test_split(self.data, test_size=0.2)\n","        self.training_samples = train_samples\n","        self.validation_samples = validation_samples\n","        return None\n","    \n","    def generator(self, samples, batch_size=32, b_image_generate=1):\n","        # generator_iter = 30\n","        num_samples = len(samples)\n","        while 1: # Loop forever so the generator never terminates\n","            shuffle(samples)\n","            for offset in range(0, num_samples, batch_size):\n","                batch_samples = samples[offset:offset+batch_size]\n","                layer_outputs = []\n","                groundtruth_bounds = []\n","                for batch_sample in batch_samples: \n","                    # groundtruth_bound = [0] * 3 \n","                    groundtruth_bound = []\n","                    for i in range(3):\n","                        obj = batch_sample[i]\n","                        groundtruth_bound.append(obj[1])\n","                  \n","                    groundtruth_bounds.append(groundtruth_bound)\n","                    # load the image and get output of yolov layer n\n","                    # pdb.set_trace()\n","                    split_name = batch_sample[0][0].split('/')\n","                    name = self.base_path + '/' + split_name[0] + '/' + split_name[1] + '_image.jpg'\n","                    image = cv2.imread(name)\n","                    if image is not None:\n","                        image_h, image_w, _ = image.shape\n","                        process_image = preprocess_input(image, net_h, net_w)\n","                        # layer_output = get_98th_layer_output([process_image])[0]\n","                        layer_output = intermediate_layer_model.predict([process_image])[0]\n","                        layer_outputs.append(layer_output)\n","                        intermediate_layer_model\n","                # FINAL \n","                # pdb.set_trace()\n","                x_train = np.array(layer_outputs)\n","                y_train = np.array(groundtruth_bounds)\n","                yield (x_train, y_train)\n","                \n","    def train_generator(self, batch_size):\n","        return self.generator(samples = self.training_samples, batch_size = self.batch_size, b_image_generate=1)\n","    def validation_generator(self, batch_size):\n","        return self.generator(samples = self.validation_samples, batch_size = self.batch_size, b_image_generate=0)\n","    \n","    def run(self):\n","        self.split_data()\n","        checkpoint = ModelCheckpoint(self.model_path + self.model_name + 'best.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)\n","        callbacks_list = [checkpoint]\n","        train_len = len(self.training_samples)\n","        print(train_len)\n","        valid_len = len(self.validation_samples)\n","        print(valid_len)\n","        history = self.model.fit_generator(self.train_generator(self.batch_size),\n","            steps_per_epoch=ceil(train_len/self.batch_size),\n","            validation_data=self.validation_generator(self.batch_size),\n","            validation_steps=ceil(valid_len/self.batch_size),\n","            callbacks=callbacks_list,\n","            epochs=self.epochs, \n","            verbose=1)\n","        self.model.save(self.model_path + self.model_name + '.h5')\n","        return (history)\n","    \n","    def plotError(self, history):\n","      # Get training and test loss histories\n","      training_loss = history.history['loss']\n","      test_loss = history.history['val_loss']\n","      # Create count of the number of epochs\n","      epoch_count = range(1, len(training_loss) + 1)\n","      # Visualize loss history\n","      plt.plot(epoch_count, training_loss, 'r--')\n","      plt.plot(epoch_count, test_loss, 'b-')\n","      plt.legend(['Training Loss', 'Test Loss'])\n","      plt.xlabel('Epoch')\n","      plt.ylabel('Loss')\n","      plt.show();  \n","      plt.savefig(self.model_path + self.model_name + '.jpg')\n","      plt.close()\n","      \n","def main():\n","    # -------------- Define the inputs to the Pipeline -------------- [S]\n","    model = feedforward_network()\n","    base_path = 'data/data-2019/trainval'\n","    # -------------- Define the inputs to the Pipeline -------------- [E]\n","    pipeline = Pipeline(model, base_path)\n","    pipeline.model_name = ('model' + str(0))\n","    pipeline.getSamples()\n","    history = pipeline.run()\n","    pipeline.plotError(history)\n","    \n","if __name__ == '__main__':\n","    main()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Running getSamples functions\n","3184\n","796\n","Epoch 1/40\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\n","<tf.Tensor 'VarIsInitializedOp_48:0' shape=() dtype=bool>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\", line 332, in predict_loop\n","    batch_outs = f(ins_batch)  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2959, in __call__\n","    if hasattr(get_session(), '_make_callable_from_options'):  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 216, in get_session\n","    [tf.is_variable_initialized(v) for v in candidate_vars])  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 216, in <listcomp>\n","    [tf.is_variable_initialized(v) for v in candidate_vars])  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n","    return _add_should_use_warning(fn(*args, **kwargs))\n","==================================\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\n","<tf.Tensor 'VarIsInitializedOp_47:0' shape=() dtype=bool>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\", line 332, in predict_loop\n","    batch_outs = f(ins_batch)  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2959, in __call__\n","    if hasattr(get_session(), '_make_callable_from_options'):  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 216, in get_session\n","    [tf.is_variable_initialized(v) for v in candidate_vars])  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 216, in <listcomp>\n","    [tf.is_variable_initialized(v) for v in candidate_vars])  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n","    return _add_should_use_warning(fn(*args, **kwargs))\n","==================================\n","24/25 [===========================>..] - ETA: 15s - loss: 14921.3868 - mean_squared_error: 28857.6846Epoch 1/40\n"," 7/25 [=======>......................] - ETA: 4:06 - loss: 258.1169 - mean_squared_error: 252.7477\n","Epoch 00001: val_loss improved from inf to 258.11687, saving model to models/model0best.h5\n","25/25 [==============================] - 502s 20s/step - loss: 14356.0809 - mean_squared_error: 27747.2429 - val_loss: 258.1169 - val_mean_squared_error: 252.7477\n","Epoch 2/40\n","24/25 [===========================>..] - ETA: 9s - loss: 422.8617 - mean_squared_error: 534.6612 Epoch 1/40\n"," 7/25 [=======>......................] - ETA: 4:06 - loss: 240.8534 - mean_squared_error: 241.3113\n","Epoch 00002: val_loss improved from 258.11687 to 240.85338, saving model to models/model0best.h5\n","25/25 [==============================] - 354s 14s/step - loss: 417.9655 - mean_squared_error: 525.7285 - val_loss: 240.8534 - val_mean_squared_error: 241.3113\n","Epoch 3/40\n","24/25 [===========================>..] - ETA: 10s - loss: 283.5771 - mean_squared_error: 296.0176Epoch 1/40\n"," 7/25 [=======>......................] - ETA: 4:05 - loss: 240.8709 - mean_squared_error: 238.6711\n","Epoch 00003: val_loss did not improve from 240.85338\n","25/25 [==============================] - 357s 14s/step - loss: 282.5154 - mean_squared_error: 294.9885 - val_loss: 240.8709 - val_mean_squared_error: 238.6711\n","Epoch 4/40\n","24/25 [===========================>..] - ETA: 10s - loss: 265.4296 - mean_squared_error: 268.6146Epoch 1/40\n"," 7/25 [=======>......................] - ETA: 4:02 - loss: 231.9275 - mean_squared_error: 234.4371\n","Epoch 00004: val_loss improved from 240.85338 to 231.92752, saving model to models/model0best.h5\n","25/25 [==============================] - 364s 15s/step - loss: 264.8534 - mean_squared_error: 268.3305 - val_loss: 231.9275 - val_mean_squared_error: 234.4371\n","Epoch 5/40\n","24/25 [===========================>..] - ETA: 10s - loss: 260.9353 - mean_squared_error: 263.1940Epoch 1/40\n"," 7/25 [=======>......................] - ETA: 4:02 - loss: 236.9953 - mean_squared_error: 237.6144\n","Epoch 00005: val_loss did not improve from 231.92752\n","25/25 [==============================] - 352s 14s/step - loss: 261.8364 - mean_squared_error: 263.0815 - val_loss: 236.9953 - val_mean_squared_error: 237.6144\n","Epoch 6/40\n","24/25 [===========================>..] - ETA: 10s - loss: 253.3366 - mean_squared_error: 256.8787Epoch 1/40\n"," 7/25 [=======>......................] - ETA: 4:00 - loss: 234.0918 - mean_squared_error: 229.6978\n","Epoch 00006: val_loss did not improve from 231.92752\n","25/25 [==============================] - 352s 14s/step - loss: 255.7771 - mean_squared_error: 256.7721 - val_loss: 234.0918 - val_mean_squared_error: 229.6978\n","Epoch 7/40\n","24/25 [===========================>..] - ETA: 10s - loss: 248.5961 - mean_squared_error: 247.5201Epoch 1/40\n"," 5/25 [=====>........................] - ETA: 4:12 - loss: 210.7177 - mean_squared_error: 218.2955"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vV4X3Cqllk_n","colab_type":"text"},"source":["# Failed Model T.T Using 15 Objects and 8 vertcies of x, y, z"]},{"cell_type":"code","metadata":{"id":"1GQJ6JWhIDfB","colab_type":"code","colab":{}},"source":["import numpy as np\n","import csv\n","import cv2\n","import sklearn\n","import matplotlib.pyplot as plt\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras_preprocessing.image import ImageDataGenerator\n","from keras.preprocessing.image import img_to_array\n","from sklearn.model_selection import train_test_split\n","from random import shuffle\n","from math import ceil\n","from IPython.core.debugger import set_trace\n","from sklearn import preprocessing\n","from numpy import expand_dims\n","import pdb\n","\n","class Pipeline:\n","    def __init__(self, model=None, base_path=''):\n","        self.data = []\n","        self.model = model\n","        self.base_path = base_path\n","        self.epochs = 20\n","        self.batch_size = 128\n","        self.training_samples = []\n","        self.validation_samples = []\n","        self.csv_path = self.base_path + '/3D_bound_boxes.csv'\n","        self.model_path = 'models/'\n","\n","    def getSamples(self):\n","        print('Running getSamples functions')\n","        with open(self.csv_path) as csvfile:\n","            reader = csv.reader(csvfile)\n","            next(reader)\n","            count = 0\n","            lines = []\n","            for line in reader:\n","                lines.append(line)\n","                count += 1\n","                if (count == 15):\n","                    count = 0\n","                    self.data.append(lines)\n","                    lines = []\n","        return None\n","    \n","    def split_data(self):\n","        train_samples, validation_samples = train_test_split(self.data, test_size=0.2)\n","        self.training_samples = train_samples\n","        self.validation_samples = validation_samples\n","        return None\n","    \n","    def generator(self, samples, batch_size=32, b_image_generate=1):\n","        # generator_iter = 30\n","        num_samples = len(samples)\n","        while 1: # Loop forever so the generator never terminates\n","            shuffle(samples)\n","            for offset in range(0, num_samples, batch_size):\n","                batch_samples = samples[offset:offset+batch_size]\n","                layer_outputs = []\n","                groundtruth_bounds = []\n","                for batch_sample in batch_samples:  \n","                    # groundtruth_bound = [1] * 24 * 15 # (x,y,z) * 8 * 15\n","                    groundtruth_bound = []\n","                    for i in range(len(batch_sample)):\n","                        obj = batch_sample[i]\n","                        groundtruth_bound.append(obj[2])\n","                        groundtruth_bound.append(obj[3])\n","                        groundtruth_bound.append(obj[4])\n","                        groundtruth_bound.append(obj[5])\n","                        groundtruth_bound.append(obj[6])\n","                        groundtruth_bound.append(obj[7])\n","                        groundtruth_bound.append(obj[8])\n","                        groundtruth_bound.append(obj[9])\n","                        groundtruth_bound.append(obj[10])\n","                        groundtruth_bound.append(obj[11])\n","                        groundtruth_bound.append(obj[12])\n","                        groundtruth_bound.append(obj[13])\n","                        groundtruth_bound.append(obj[14])\n","                        groundtruth_bound.append(obj[15])\n","                        groundtruth_bound.append(obj[16])\n","                        groundtruth_bound.append(obj[17])\n","                        groundtruth_bound.append(obj[18])\n","                        groundtruth_bound.append(obj[19])\n","                        groundtruth_bound.append(obj[20])\n","                        groundtruth_bound.append(obj[21])\n","                        groundtruth_bound.append(obj[22])\n","                        groundtruth_bound.append(obj[23])\n","                        groundtruth_bound.append(obj[24])\n","                        groundtruth_bound.append(obj[25])\n","                        \n","                    groundtruth_bounds.append(groundtruth_bound)\n","                    # load the image and get output of yolov layer n\n","                    split_name = batch_sample[0][0].split('_')[0]\n","                    name = self.base_path + '/' + split_name + '_image.jpg'\n","                    image = cv2.imread(name)\n","                    if image is not None:\n","                        image_h, image_w, _ = image.shape\n","                        process_image = preprocess_input(image, net_h, net_w)\n","                        layer_output = get_98th_layer_output([process_image])[0]\n","                        # layer_output = yolov3([process_image])[0]\n","                        layer_outputs.append(layer_output)\n","                # FINAL \n","                # pdb.set_trace()\n","                x_train = np.array(layer_outputs)\n","                y_train = np.array(groundtruth_bounds)\n","                yield (x_train, y_train)\n","                \n","    def train_generator(self, batch_size):\n","        return self.generator(samples = self.training_samples, batch_size = self.batch_size, b_image_generate=1)\n","    def validation_generator(self, batch_size):\n","        return self.generator(samples = self.validation_samples, batch_size = self.batch_size, b_image_generate=0)\n","    \n","    def run(self):\n","        self.split_data()\n","        checkpoint = ModelCheckpoint(self.model_path + self.model_name + 'best.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='max', period=1)\n","        callbacks_list = [checkpoint]\n","        train_len = len(self.training_samples)\n","        print(train_len)\n","        valid_len = len(self.validation_samples)\n","        print(valid_len)\n","        history = self.model.fit_generator(self.train_generator(self.batch_size),\n","            steps_per_epoch=ceil(train_len/self.batch_size),\n","            validation_data=self.validation_generator(self.batch_size),\n","            validation_steps=ceil(valid_len/self.batch_size),\n","            callbacks=callbacks_list,\n","            epochs=self.epochs, \n","            verbose=1)\n","        self.model.save(self.model_path + self.model_name + '.h5')\n","        return (history)\n","    \n","    def plotError(self, history):\n","      # Get training and test loss histories\n","      training_loss = history.history['loss']\n","      test_loss = history.history['val_loss']\n","      # Create count of the number of epochs\n","      epoch_count = range(1, len(training_loss) + 1)\n","      # Visualize loss history\n","      plt.plot(epoch_count, training_loss, 'r--')\n","      plt.plot(epoch_count, test_loss, 'b-')\n","      plt.legend(['Training Loss', 'Test Loss'])\n","      plt.xlabel('Epoch')\n","      plt.ylabel('Loss')\n","      plt.show();  \n","      plt.savefig(self.model_path + self.model_name + '.jpg')\n","      plt.close()\n","      \n","def main():\n","    # -------------- Define the inputs to the Pipeline -------------- [S]\n","    model = feedforward_network()\n","    base_path = 'data/data-2019/trainval'\n","    # -------------- Define the inputs to the Pipeline -------------- [E]\n","    pipeline = Pipeline(model, base_path)\n","    pipeline.model_name = ('model' + str(0))\n","    pipeline.getSamples()\n","    history = pipeline.run()\n","    pipeline.plotError(history)\n","    \n","if __name__ == '__main__':\n","    main()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wfwjJ2afx4m","colab_type":"text"},"source":["# Test the Code"]},{"cell_type":"code","metadata":{"id":"2iw6-JzGPUYM","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":110},"outputId":"bb7b9ef6-70eb-4919-d98f-98a18626354a","executionInfo":{"status":"ok","timestamp":1576264368741,"user_tz":300,"elapsed":3468982,"user":{"displayName":"Nam Gyu Kil","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ5TS287X08iAcQu7bCatJvBOJr9-oiAfqj5Ox=s64","userId":"08278423732697661539"}}},"source":["%cd /content/drive/My Drive/Colab Notebooks/VehicleDetection\n","%cd /content/drive/My Drive/Colab Notebooks/VehicleDetection/keras-yolo3-master\n","from google.colab import files\n","files.upload()  #this will prompt you to upload the kaggle.json"],"execution_count":48,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/VehicleDetection\n","/content/drive/My Drive/Colab Notebooks/VehicleDetection/keras-yolo3-master\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-0071d870-e47d-4f5c-8369-a87921633b12\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-0071d870-e47d-4f5c-8369-a87921633b12\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving gmail_shorterlabels_10.h5 to gmail_shorterlabels_10 (1).h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HmP3zJAue-cK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"775e4858-39fb-4a86-f081-f85782320155","executionInfo":{"status":"ok","timestamp":1576260259882,"user_tz":300,"elapsed":10169,"user":{"displayName":"Nam Gyu Kil","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ5TS287X08iAcQu7bCatJvBOJr9-oiAfqj5Ox=s64","userId":"08278423732697661539"}}},"source":["import tensorflow as tf\n","from keras.models import load_model, Model\n","import keras\n","# model = load_model('models/Bound3D_model.h5')\n","# model = tf.keras.models.load_model('models/Bound3D_model.h5')\n","model = tf.keras.models.load_model('models/model0best.h5')"],"execution_count":40,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4iUKf7XfFvh","colab_type":"code","colab":{}},"source":["from google.colab.patches import cv2_imshow\n","photo_filename = 'data4YOLO/image_folder/00107_image.jpg'\n","photo_filename = 'data4YOLO/image_folder/00020_image.jpg'\n","# photo_filename = 'data4YOLO/image_folder/00058_image.jpg'\n","# photo_filename = 'data4YOLO/image_folder/04458_image.jpg'\n","# load and prepare image\n","image = cv2.imread(photo_filename)\n","image_h, image_w, _ = image.shape\n","\n","cv2_imshow(image)\n","process_image = preprocess_input(image, net_h, net_w)\n","layer_output = get_98th_layer_output([process_image])[0]\n","output = model.predict(layer_output)\n","print(output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTr8QRd9fG-K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d02f4263-f628-40d6-d109-75c04e2e3094","executionInfo":{"status":"ok","timestamp":1576244789636,"user_tz":300,"elapsed":1379,"user":{"displayName":"Nam Gyu Kil","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ5TS287X08iAcQu7bCatJvBOJr9-oiAfqj5Ox=s64","userId":"08278423732697661539"}}},"source":["print(len(output[0]))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["360\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TulgaPATgmG3","colab_type":"code","colab":{}},"source":["import numpy as np\n","from glob import glob\n","import csv\n","from random import randint\n","import math\n","\n","predictions = []\n","count = 0\n","with open('template.csv') as csvfile:\n","    # set_trace()\n","    reader = csv.reader(csvfile)\n","    next(reader)\n","    for line in reader:\n","        if count % 50 == 0:\n","            print(count)\n","        else:\n","            print(count, end=\" \")\n","        split_name = line[0].split('/')\n","        micro_path = split_name[0] + '/' + split_name[1]\n","        image = cv2.imread('data/data-2019/test/' + micro_path + '_image.jpg')\n","        image_h, image_w, _ = image.shape\n","        process_image = preprocess_input(image, net_h, net_w)\n","        # make prediction\n","        layer_output = get_98th_layer_output([process_image])[0]\n","        output = model.predict(layer_output)\n","        # pdb.set_trace()\n","        x = output[0][0]\n","        y = output[0][1]\n","        z = output[0][2]\n","        r = math.sqrt(x**2 + y**2 + z**2)\n","        theta = np.arctan(x/z)\n","        count += 1\n","        predictions.append([r, theta])\n","        next(reader)\n","        # if (count > 5):\n","        #   break\n","    # set_trace()\n","print(predictions)\n","################################ Generating CSV ################################\n","name = '3d_bounding_output.csv'\n","cnt = 0\n","with open(name, 'w') as f:\n","    writer = csv.writer(f, delimiter=',', lineterminator='\\n')\n","    with open('template.csv') as csvfile:\n","        reader = csv.reader(csvfile)\n","        writer.writerow(['guid/image/axis', 'value'])\n","        next(reader)\n","        for line in reader:\n","            pdb.set_trace()\n","            output_type = line[0].split('/')[2]\n","            if output_type == 'r':\n","                writer.writerow([line[0], predictions[cnt][0]]) # r  \n","            else:\n","                writer.writerow([line[0], predictions[cnt][1] * np.pi * 2]) # theta\n","                cnt += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"isSsbgxkc1J-","colab_type":"code","colab":{}},"source":["################################ Generating CSV ################################\n","name = '3d_bounding_output.csv'\n","cnt = 0\n","with open(name, 'w') as f:\n","    writer = csv.writer(f, delimiter=',', lineterminator='\\n')\n","    with open('template.csv') as csvfile:\n","        reader = csv.reader(csvfile)\n","        writer.writerow(['guid/image/axis', 'value'])\n","        next(reader)\n","        for line in reader:\n","            # pdb.set_trace()\n","            output_type = line[0].split('/')[2]\n","            if output_type == 'r':\n","                writer.writerow([line[0], predictions[cnt][0]]) # r  \n","            else:\n","                writer.writerow([line[0], predictions[cnt][1] * np.pi * 2]) # theta\n","                cnt += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrotUQt7dAd7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}